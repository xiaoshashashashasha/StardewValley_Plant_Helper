import os
import tools
import chromadb
import json
import google.generativeai as genai
import streamlit as st

from typing import List
from sentence_transformers import CrossEncoder
from sentence_transformers import SentenceTransformer

# ---é…ç½®åŠ è½½æ¨¡å‹å’Œæ•°æ®åº“---
# é…ç½®GeminiAPIKey
API_KEY = os.environ.get("GOOGLE_API_KEY")
genai.configure(api_key=API_KEY)


# é…ç½®åŠ è½½Embeddingå’ŒCross-Encoderæ¨¡å‹
@st.cache_resource
def load_models():
    embedding_model = SentenceTransformer("shibing624/text2vec-base-chinese")
    cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')
    return embedding_model, cross_encoder


# è¿æ¥å‘é‡æ•°æ®åº“
@st.cache_resource
def load_chromadb():
    chromadb_client = chromadb.PersistentClient("zuowu.db")
    return chromadb_client.get_or_create_collection(name="default")


# ---å¬å›å’Œé‡æ’æ–¹æ³•---
def retrieve_and_rerank(query: str, top_k1=10, top_k2=4):
    # è·å–æé—®å‘é‡å€¼
    query_embedding = embedding.encode(query).tolist()

    # å¬å›è¿‡ç¨‹ï¼Œç²—ç•¥è¿›è¡Œæ•°æ®åº“å‘é‡åŒ¹é…
    retrieved_info = chromadb_collection.query(query_embeddings=query_embedding, n_results=top_k1)
    retrieved = retrieved_info['documents'][0]

    # é‡æ’è¿‡ç¨‹ï¼Œå¯¹å¬å›ç‰‡æ®µé€ä¸€è¿›è¡Œæ¯”è¾ƒæ‰“åˆ†
    pairs = [(query, chunk) for chunk in retrieved]
    scores = cross_encoder.predict(pairs)

    chunk_with_scores = [(chunk, score) for chunk, score in zip(retrieved, scores)]
    chunk_with_scores.sort(key=lambda pair: pair[1], reverse=True)

    return [chunk for chunk, _ in chunk_with_scores[:top_k2]]


# ---å‡½æ•°è°ƒç”¨---
def call_tool(tool_call):
    function_name = tool_call.name
    args = {k: v for k, v in tool_call.args.items()}

    # å‡½æ•°ååˆ°å®é™…å‡½æ•°çš„æ˜ å°„
    available_functions = {
        "get_crops_by_sellprice": tools.get_crops_by_sellprice,
        "get_crops_by_dailyrevenue": tools.get_crops_by_dailyrevenue,
        "get_crops_by_seedprice": tools.get_crops_by_seedprice,
        "get_crops_by_growtime": tools.get_crops_by_growtime,
    }

    if function_name in available_functions:
        return available_functions[function_name](**args)


# ---å›ç­”æ–¹æ³•---
def generate_answer(query: str):
    prompt = f"""ä½ æ˜¯ä¸€ä½æ˜Ÿéœ²è°·å†œä½œç‰©ç§æ¤åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œæä¾›ç‰‡æ®µä¸­çš„æœ‰ç”¨ä¿¡æ¯ç”Ÿæˆå‡†ç¡®å›ç­”ã€‚å›ç­”æ ¼å¼è¯·å°½é‡ç®€æ´ï¼Œç¾è§‚ã€‚
    ä¸è¦ç¼–é€ ä¿¡æ¯ï¼Œè¯·ä»å·¥å…·åº“ä¸­é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥è·å–ä¿¡æ¯ã€‚è‹¥æ— éœ€å…¶ä»–ä¿¡æ¯ï¼Œåˆ™ç›´æ¥å›ç­”ã€‚è‹¥æ‰€è·ä¿¡æ¯æ— æ³•è§£å†³é—®é¢˜ï¼Œåˆ™ç›´æ¥è¯´æ˜æ— æ³•è§£å†³ã€‚"""

    # æ„é€ ç¬¬ä¸€è½®æ¶ˆæ¯
    messages = [
        {"role": "user", "parts": [{"text": prompt}]},
        {"role": "user", "parts": [{"text": f"ç”¨æˆ·é—®é¢˜: {query}\n\n"}]}
    ]

    # è·å–å“åº”
    model = genai.GenerativeModel("gemini-2.5-flash", tools=tools.TOOLS_LIST)
    response = model.generate_content(messages)

    # æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«å‡½æ•°è°ƒç”¨
    try:
        tool_call = response.candidates[0].content.parts[0].function_call

        # è‹¥ä¸ºå‡½æ•°è°ƒç”¨
        if tool_call:
            print("get function calling\n")

            # è‹¥ä¸ºRAGæ£€ç´¢è¯·æ±‚
            if tool_call.name == "RAGCalling":
                print("RAG Search\n")

                # æ‰§è¡Œå¬å›ã€é‡æ’è¿‡ç¨‹
                retrieved_chunks = retrieve_and_rerank(user_query)
                messages.append({"role": "user", "parts": [{"text": f"å¯ç”¨ç‰‡æ®µå¦‚ä¸‹:{retrieved_chunks}"}]})

                print("ç”Ÿæˆå›ç­”ï¼Œç›¸å…³åˆ†ç‰‡ä¸º:\n")
                i = 0
                for chunk in retrieved_chunks:
                    print(f"åˆ†ç‰‡{i}:{chunk}\n")
                    i += 1

                # æ ¹æ®RAGç‰‡æ®µç”Ÿæˆæœ€ç»ˆå›ç­”
                final_response = model.generate_content(messages)
                return final_response.text

            # è‹¥ä¸ºæ­£å¸¸å‡½æ•°è°ƒç”¨
            else:
                print("Function Search\n")

                # æ‰§è¡Œå‡½æ•°å¹¶è·å–ç»“æœ
                tool_output = call_tool(tool_call)

                # æ„é€ å‡½æ•°å“åº”æ¶ˆæ¯æ ¼å¼
                function_response_part = {
                    "function_response": {
                        "name": tool_call.name,
                        "response": {
                            "content": tool_output
                        }
                    }
                }
                messages.append({"role": "function", "parts": [function_response_part]})

                # å°†å‡½æ•°ç»“æœä¼ ç»™æ¨¡å‹ç”Ÿæˆæœ€ç»ˆå›ç­”
                final_response = model.generate_content(messages)
                return final_response.text

        # è‹¥æ— éœ€é¢å¤–ä¿¡æ¯æ¥æº
        else:
            # ç›´æ¥è¿”å›å›ç­”
            print(response.text)
            return response.text
    except (AttributeError, IndexError):
        if response.text:
            return response.text


# ---æ¶ˆæ¯æŒä¹…åŒ–---
CHAT_HISTORY = "chat_history.json"


# å†å²æ¶ˆæ¯åŠ è½½æ–¹æ³•
def load_chat_history():
    if os.path.exists(CHAT_HISTORY):
        with open(CHAT_HISTORY, "r") as file:
            return json.load(file)
    return []


# å†å²æ¶ˆæ¯ä¿å­˜æ–¹æ³•
def save_chat_history(messages):
    with open(CHAT_HISTORY, "w") as file:
        json.dump(messages, file, ensure_ascii=False, indent=4)


# ---UIç•Œé¢---
st.set_page_config(page_title="æ˜Ÿéœ²è°·ç‰©è¯­å°åŠ©æ‰‹", page_icon="ğŸŒ±")
st.title("ğŸŒ±æ˜Ÿéœ²è°·ç‰©è¯­å†œä½œç‰©å°åŠ©æ‰‹")
col1, col2 = st.columns([4,1])
with col1:
    st.markdown("è¾“å…¥ä½ çš„å†œä½œç‰©ç›¸å…³é—®é¢˜ï¼Œæˆ‘å°†æ ¹æ®æœ¬åœ°çŸ¥è¯†åº“ä¸ºä½ æä¾›å‡†ç¡®çš„æ”»ç•¥ä¿¡æ¯ã€‚")

with col2:
    # æ·»åŠ æ¸…ç©ºå†å²çš„æŒ‰é’®
    if st.button("æ¸…ç©ºå†å²æ¶ˆæ¯"):
        st.session_state.messages = []
        save_chat_history([])
        st.rerun()

# åˆå§‹åŒ–æ¨¡å‹å¹¶è¿æ¥çŸ¥è¯†åº“
with st.spinner("æ­£åœ¨åŠ è½½Embeddingå’ŒCross-Encoderæ¨¡å‹..."):
    embedding, cross_encoder = load_models()

with st.spinner("æ­£åœ¨è¿æ¥çŸ¥è¯†åº“..."):
    chromadb_collection = load_chromadb()

# ---é¡µé¢ä¼šè¯é€»è¾‘---
# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ä¸­çš„èŠå¤©è®°å½•
if "messages" not in st.session_state:
    with st.spinner("æ­£åœ¨åŠ è½½å†å²æ¶ˆæ¯..."):
        st.session_state.messages = load_chat_history()

# éå†å¹¶æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç›‘å¬ç”¨æˆ·è¾“å…¥
if user_query := st.chat_input("åœ¨è¿™é‡Œè¾“å…¥ä½ çš„é—®é¢˜..."):
    # åœ¨ä¼šè¯ä¸­æ·»åŠ æ–°ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": user_query})

    # æ¸²æŸ“æ–°ç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(user_query)

    # æ¸²æŸ“åŠ©æ‰‹æ¶ˆæ¯
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æœç´¢å’Œç”Ÿæˆå›ç­”..."):
            # è°ƒç”¨å›ç­”æ–¹æ³•
            answer = generate_answer(user_query)

            # å°†å›ç­”ä¿å­˜åˆ°ä¼šè¯
            st.session_state.messages.append({
                "role": "assistant",
                "content": answer,
            })

            # ç«‹å³ä¿å­˜
            save_chat_history(st.session_state.messages)

        st.rerun()

