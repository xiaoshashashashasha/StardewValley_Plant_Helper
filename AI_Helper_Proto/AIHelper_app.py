import os
import time
import chromadb
import json
import google.generativeai as genai
import streamlit as st

from typing import List
from sentence_transformers import CrossEncoder
from sentence_transformers import SentenceTransformer

# ---é…ç½®åŠ è½½æ¨¡å‹å’Œæ•°æ®åº“---
# é…ç½®GeminiAPIKey
API_KEY = os.environ.get("GOOGLE_API_KEY")
genai.configure(api_key=API_KEY)


# é…ç½®åŠ è½½Embeddingå’ŒCross-Encoderæ¨¡å‹
@st.cache_resource
def load_models():
    embedding_model = SentenceTransformer("shibing624/text2vec-base-chinese")
    cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')
    return embedding_model, cross_encoder


# è¿æ¥å‘é‡æ•°æ®åº“
@st.cache_resource
def load_chromadb():
    chromadb_client = chromadb.PersistentClient("zuowu.db")
    return chromadb_client.get_or_create_collection(name="default")


# ---å¬å›å’Œé‡æ’æ–¹æ³•---
def retrieve_and_rerank(query: str, top_k1=10, top_k2=4):
    # è·å–æé—®å‘é‡å€¼
    query_embedding = embedding.encode(query).tolist()

    # å¬å›è¿‡ç¨‹ï¼Œç²—ç•¥è¿›è¡Œæ•°æ®åº“å‘é‡åŒ¹é…
    retrieved_info = chromadb_collection.query(query_embeddings=query_embedding, n_results=top_k1)
    retrieved = retrieved_info['documents'][0]

    # é‡æ’è¿‡ç¨‹ï¼Œå¯¹å¬å›ç‰‡æ®µé€ä¸€è¿›è¡Œæ¯”è¾ƒæ‰“åˆ†
    pairs = [(query, chunk) for chunk in retrieved]
    scores = cross_encoder.predict(pairs)

    chunk_with_scores = [(chunk, score) for chunk, score in zip(retrieved, scores)]
    chunk_with_scores.sort(key=lambda pair: pair[1], reverse=True)

    return [chunk for chunk, _ in chunk_with_scores[:top_k2]]


# ---å›ç­”æ–¹æ³•---
def generate_answer(query: str, chunks: List[str]):
    prompt = f"""ä½ æ˜¯ä¸€ä½æ˜Ÿéœ²è°·å†œä½œç‰©ç§æ¤åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œä¸‹åˆ—ç‰‡æ®µä¸­çš„æœ‰ç”¨ä¿¡æ¯ç”Ÿæˆå‡†ç¡®å›ç­”ã€‚

    ç”¨æˆ·é—®é¢˜:{query}

    ç›¸å…³ç‰‡æ®µ:
    {"\n\n".join(chunks)}

    è¯·åŸºäºä»¥ä¸Šå†…å®¹ä½œç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚"""

    model = genai.GenerativeModel("gemini-2.5-flash")

    response = model.generate_content(prompt)

    return response.text


# ---æ¶ˆæ¯æŒä¹…åŒ–---
CHAT_HISTORY = "chat_history.json"


# å†å²æ¶ˆæ¯åŠ è½½æ–¹æ³•
def load_chat_history():
    if os.path.exists(CHAT_HISTORY):
        with open(CHAT_HISTORY, "r") as file:
            return json.load(file)
    return []


# å†å²æ¶ˆæ¯ä¿å­˜æ–¹æ³•
def save_chat_history(messages):
    with open(CHAT_HISTORY, "w") as file:
        json.dump(messages, file, ensure_ascii=False, indent=4)


# ---UIç•Œé¢---
st.set_page_config(page_title="æ˜Ÿéœ²è°·ç‰©è¯­å°åŠ©æ‰‹", page_icon="ğŸŒ±")
st.title("ğŸŒ±æ˜Ÿéœ²è°·ç‰©è¯­å†œä½œç‰©å°åŠ©æ‰‹")
st.markdown("è¾“å…¥ä½ çš„å†œä½œç‰©ç›¸å…³é—®é¢˜ï¼Œæˆ‘å°†æ ¹æ®æœ¬åœ°çŸ¥è¯†åº“ä¸ºä½ æä¾›å‡†ç¡®çš„æ”»ç•¥ä¿¡æ¯ã€‚")

# åˆå§‹åŒ–æ¨¡å‹å¹¶è¿æ¥çŸ¥è¯†åº“
with st.spinner("æ­£åœ¨åŠ è½½Embeddingå’ŒCross-Encoderæ¨¡å‹..."):
    embedding, cross_encoder = load_models()

with st.spinner("æ­£åœ¨è¿æ¥çŸ¥è¯†åº“..."):
    chromadb_collection = load_chromadb()

# ---é¡µé¢ä¼šè¯é€»è¾‘---
# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ä¸­çš„èŠå¤©è®°å½•
if "messages" not in st.session_state:
    with st.spinner("æ­£åœ¨åŠ è½½å†å²æ¶ˆæ¯..."):
        st.session_state.messages = load_chat_history()

# éå†å¹¶æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

        # æ˜¾ç¤ºå›ç­”æ‰€ç”¨åˆ†ç‰‡
        if message["role"] == "assistant" and "chunks" in message:
            with st.expander("æŸ¥çœ‹ç”¨äºç”Ÿæˆç­”æ¡ˆçš„ä¸Šä¸‹æ–‡"):
                for i, chunk in enumerate(message["chunks"]):
                    st.write(f"**åˆ†ç‰‡ {i + 1}:**")
                    st.write(chunk)

# ç›‘å¬ç”¨æˆ·è¾“å…¥
if user_query := st.chat_input("åœ¨è¿™é‡Œè¾“å…¥ä½ çš„é—®é¢˜..."):
    # 1. åœ¨ä¼šè¯ä¸­æ·»åŠ æ–°ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": user_query})

    # 2. æ¸²æŸ“æ–°ç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(user_query)

    # 3. æ¸²æŸ“åŠ©æ‰‹æ¶ˆæ¯
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æœç´¢å’Œç”Ÿæˆå›ç­”..."):
            retrieved_chunks = retrieve_and_rerank(user_query)
            answer = generate_answer(user_query, retrieved_chunks)

            # å°†å›ç­”å’Œåˆ†ç‰‡ä¿å­˜åˆ°ä¼šè¯
            st.session_state.messages.append({
                "role": "assistant",
                "content": answer,
                "chunks": retrieved_chunks
            })

            # ç«‹å³ä¿å­˜
            save_chat_history(st.session_state.messages)

        st.rerun()
